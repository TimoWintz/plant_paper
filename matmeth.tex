\tim{\paragraph{Target applicative setup.}
To acquire plant images in a real world application scenario, we use a home made solution as a 3D
scanner. It consists in a X-Carve~\cite{xcarve} CNC Cartesian arm, combined with a
custom-made pan-tilt camera mount. This setup therefore has five degrees of freedom.
A Sony RX0 \alien{RGB }{}camera \alien{with $1920 \times 1060$ resolution}{} is mounted on the camera mount and is controlled using a
Wifi interface. This allows for views from any point in space in a box containing
the imaged plant. Figure~TODO illustrate resulting images from the described
setup.}{}

\paragraph{\tim{Generative plant models.}{Virtual plants.}}

The \tim{generative plant models}{virtual plants} were designed with
OpenAlea~\cite{pradal2009plantgl}. OpenAlea is an open source software
suite developped by INRIA to provide tools for plant architecture
modelling. \tim{The model plant \emph{Arabidopsis Thaliana} was chosen to
illustrate the methods in the paper, because it is a well-studied
plant and is a subject of active interest for many biologists.}{}

\tim{A generative model of \emph{A. Thaliana} was specifically
written in the L-py programming language. L-py is a python extension,
part of OpenAlea, which implements L-systems.}{3D meshes of \emph{A.
Thaliana} were designed with the L-Py library from OpenAlea, which
is a python implementation of L-system.} L-systems were developped
in 1968 by Aristid Lindenmayer~\cite{prusinkiewicz2012algorithmic}
to model plant growth.  It is a generative grammar that allows to
grow a virtual plant using symbols, shapes and constraints derived
\tim{from}{for} plant growth observation.

Formally it is called a rewriting system, or formal grammar. It
comprises:

\begin{itemize}
    \item A vocabulary V containing the \emph{modules} of the
    system. For plant generation it will represent an architectural
    element of the plant (apex, internode, leaf) and associated
    parameters (age, length, etc) \item An initial \emph{axiom}
    or state $s_0$ corresponding to the virtual plant at $t_0$. It
    is a string of elements from the vocabulary.  \item A set
    of \emph{production} rules to iterate in order to model
    the growth. They will be applied in parallel to each variable
    element from the string of the previous state. They are composed
    of a \emph{predecessor}, to identify the elements that will be
    replaced by a \emph{successor}.
\end{itemize}

The strings can then be represented graphically in 3D using OpenAlea's
PlantGL (Figure\ref{fig:lpy}). The A. Thaliana models comprised 5
different types of organs: fruit, stem, peduncle, leaf and flower.

\begin{figure}[h!]
    \centering \includegraphics[width =
    0.1\linewidth]{figures/blank.png} \caption{Left: Example of an
    L-Py system with two modules, right: Visual representation of the
    first five steps of the L-system comprising apex (green dots)
    and internodes (brown sticks) in the vocabulary. Figure from
    \cite{boudon_l-py:_2012}} \label{fig:lpy}
\end{figure}

\tim{TODO: I think if we want to explain L-system, we have to
provide a simple example with the associated rewriting rules}{} \alien{pseudo-code of arabidopsis generation and corresponding images}{}

\tim{Randomness can be added to the models by assigining different
probabilities to the set for rules. This allow to produce different
looking individual plants from a single set of rules. Our L-py model
for \emph{A. Thaliana} can be found online~\cite{}.}{}

\tim{The generated plant models are exported as meshes using the
standard OBJ format.}{}


\paragraph{Virtual scanner.} \tim{The open source software
Blender~\cite{blender} was chosen for the rendering part. The blender
Eevee renderer allows for fast and realistic looking rendering of
scenes and the python bpy modules allows for easy scripting.}{}

\tim{The idea is to provide a simulated environment similar to
the real application described above.}{} \tim{A python script using bpy and
flask}{An API} was \tim{written}{implemented} to
generate and visualise the generated plant 3D models in Blender.
\tim{HTTP requests allow us to easily load different plant models and
backgrounds into the Blender scene, to move the loaded object,
as well as change the camera position, rotation and its intrinsic
parameters. The rendering of the scene into an image can also
be triggered by a simple request. This enables us to take pictures of the
model around the plant, as we would on the real world setup~(Figure
\ref{fig:plants}).}{The 3D models are loaded in a virtual environment
and virtual cameras are used to take pictures of the
model~(Figure \ref{fig:plants}).}

\tim{}{The API was used to simulate a virtual scanner and generate images
for the training of neural networks. The focal, position, pan and
tilt of the cameras are tunable and allow to generate images from
different views. It is also possible to move the plant in the scanner.}

\tim{The scene background }{The plant is placed in a virtual background. It} is a $360^{\circ}$
\tim{real world picture}{image} and rendering it in blender reproduces the
original scene lighting\tim{. In order to increase the complexity of the
images acquired with the virtual scanner, s}{S}everal sets of backgrounds were
used, without limitations on the lighting environement -- night, daylight,
sunset -- or the type of scene -- indoor or outdoor.


\begin{figure}[h]
    \centering \includegraphics[width=0.1\linewidth]{figures/blank.png}
    \caption{Illustration of 2D overlapping due to perspective
    projection } \label{fig:pinhole}
\end{figure}

The \tim{ground truth plant part segmentation is}{virtual labels are} acquired by rendering the plant material by material, with one material per
\tim{plant part}{organ}~(Figure \ref{fig:plants}). The
\tim{plant parts}{organs} considered for \emph{A. Thaliana} are leaf, stem, flower,
fruit and peduncle. \tim{As we will see later for the 3D reconstruction, i}{I}n the pinhole model \cite{} we use for 3D to 2D
projection, a whole line in 3D projects onto a single pixel (Figure
\ref{fig:pinhole}). As a consequence, a single pixel can belong to
several classes depending on the organs crossed by the line. The
labelling method takes this plurality into account by generating a
ground truth image for each class.

\begin{figure}[h]
    \centering \includegraphics[width=0.1\linewidth]{figures/blank.png}
    \caption{Example of virtual training images and all labels}
    \label{fig:plants}
\end{figure}

TODO: add change text after Christophe new plants, add random colors

\tim{The virtual scanner was used to generate a training dataset of plant
images with plant part segmentation 
for the training of neural networks.}{}


% \paragraph{3D ground truth.} The 3D virtual plants were
% used to train and evaluate the 3D reconstruction. The
% virtual mesh is regularly subsampled with CloudCompare
% \cite{girardeau-montaut_cloudcompare-open_2011} in order to get
% a density matching the voxel density in the volume to carve (see
% space carving section for explanations). Then this 3D point cloud is
% voxellized and each voxel of the volume to carve acquires an organ
% class, or class 0 (background) if it is not part of the plant. Then
% the vector is saved in a sparse representation to save memory. For the
% trainings, the data generator reads the sparse vector and encodes it
% in a dense representation to compare it to the predicted classes. It
% is needed to densify it because PyTorch can't backpropagate easily
% through a sparse representation.

%To train the classification on real plants, it was first considered
% to print a known plant model in 3D with Sculpteo, however the stem of
% the model wasn't thick enough and it buckled during the printing. We
% also considered to use an opensource dataset, however most segmented 3D
% dataset aimed for reconstruction are provided in Lidar or RGB-D data,
% and not from RGB to 3D. Therefore we decided to restrict ourselves to
% 3D virtual plants. We also used the 3D reconstructed models with the
% initial space-carving approach implemented previously in the pipeline
% \cite{wintz_automated_nodate} for comparison.


\paragraph{Image \tim{semantic}{} segmentation.} The objective of image \tim{semantic}{} segmentation is
to convert an input image in a stack of probability maps of the same
dimension as the image. \tim{If $C$ is the number of different classes,
semantic segmentation produces $C$ output images with values between $0$ and $1$,
giving for each pixel of the input image the probability of that pixel
belonging to the corresponding class.}{For C classes there will be C probability
maps, each corresponding to one class. The value of a pixel in the
probability map of class k is the probability that this pixel belongs
to the organ-type k in the original image.}


\tim{To produce such semantic probability maps, we used a segmentation
convolutional neural network~\cite{guo_review_2018}}{The image segmentation was performed using segmentation convolutional
neural network \cite{guo_review_2018}}. \tim{These}{Such} segmentation networks
are based on a contracting structure from an image to a low dimension\tim{al}{}
feature space, \tim{called the \emph{latent space}}{}, which encodes the content of the image. It is branched
to a symmetric expanding structure that translates the information from
the feature space to an image similar to the original one, but with
only the content of interest reconstructed. The contracting structure
is directly inspired from classification neural networks, and made
of convolution layers, non-linearities and down-pooling layers. The
convolution kernels allow to filter the information to emphasize
the content, and the down-pooling allows to reduce the dimension
of the information. The expanding structure reproduces the path in
the other direction, with up-pooling layers and convolutions. The
spatial information is re-injected to reconstruct the image properly,
by providing the down-pooling coordinates from the contracting phase.

\begin{figure}[h!]
    \centering \includegraphics[scale = 0.25]{figures/blank.png}
    \caption{Example of segmentation network: the U-Net architecture
    \cite{ronneberger_u-net:_2015}} \label{fig:unet}
\end{figure}

\tim{We decided to use a neural network architecture inspired by}{The structure tested was inspired from}
U-Net~\cite{ronneberger_u-net:_2015}. However \tim{, to leverage the power of
already existing labeled datasets}{}, the contracting
structure, or encoder, was replaced by a classification network trained
on ImageNet, and with six classes to segment. \tim{Thanks to the great diversity
in the ImageNet dataset, t}{T}his classification
structure has already \tim{learned}{been trained} to encode the semantic
representation of an image in the latent space. \tim{The classification network that we used is}{We tested}
ResNet~\cite{he_deep_2015} which is a deep neural network where inputs
from previous layers are regularly reinjected into deeper layers
in order to maintain the geometry and avoid vanishing gradients
\cite{hochreiter_vanishing_1998}.

\paragraph{Deep learning training} The network was trained with
images from the virtual scanner. TODO(Describe the path, dimension
of the images). The dataset comprised TODO(Number of images). To make
the network robust to images and lignting conditions that are not in
the dataset, we artificially augment the dataset by adding TODO(add
gaussian noise, change contrast, rotation, déformation, more?).
The dataset was split into 3 sets:

\begin{itemize}
    \item a training set to train the network (50\% of the dataset)
    \item a validation set on which the network is not trained and
    is used to evaluate and compare the different networks (25\%
    of the dataset) \item a test set used at the very end on the
    selected architecture (25\% of the dataset)
\end{itemize}


To evaluate the segmentation of the images, a combination of
metrics were used. As it is a multiclass problem, a sigmoid is
applied to each output in order to contain the numerical range of
the predictions. First crossentropy was used, it is a per-pixel
metric. It represents the uncertainty of the prediction compared to
the ground-truth. If the class of a pixel has a very low predicted
probability, it will highly penalise the loss. If the probability
is close to one, the contribution to the loss is close to zero. The
notion of entropy represents this discrepancy between the ground truth
distribution and the predicted distribution. It is naturally translated
at the mathematical level with the negative of the logarithm.

\begin{equation}
    L =-\sum_i^n\sum_k^C{y^k_{i, gt}\ln(y_{\textrm{i},
    \textrm{pred}}^k)}
\end{equation}

Where n is the number of pixels in the image, C the number of possible
classes, $y_{gt}^k = 1$ if pixel i is in class k, 0 otherwise, and
$y_ {\textrm{i}, \textrm{pred}}^k = p(\textrm{label}(y_{\textrm{i},
\textrm{gt}}) = k)$.

The second loss we used was the Dice coefficient, which theoretically
writes as:

\begin{equation}
    s = 1 - \frac{1}{n}\sum_{i=1}^n\sum_{k=0}^{C}y_{\textrm{i},
    \textrm{gt}} y_{\textrm{i}, \textrm{pred}}^k
\end{equation}


This coefficient compares the number of right predictions to the
number of samples, for each class. In practice, the product of the
ground truth by the predictions is summed, so that only the prediction
of the right class contributes to the sum. For example for point n,
the class is k, and the prediction for class k is $p_k$, the loss
for point n will be $1 - \frac{2p_k}{2} = 1 - p_k$. The total loss
is computed vector-wise, and lies between 0 and 1.


We used the mean of these two losses for the training, as the
crossentropy has more stable gradients, but the Dice loss is what we
want to minimise, and is less sensitive to class imbalance.


\paragraph{Fine-tuning.} Our network was trained on virtual \emph{A.
Thaliana} model but aims at reconstructing real plants. Therefore when
the virtual plants fail at reproducing the complexity of real plants,
the network can perform poorly. the network also fails when confronted
to other plant species with very different traits. Therefore we
conceived a simple interface that allows the user to manually label a
few images of interest (three images is enough), then run the training
of the network on this small dataset. This way, the network will be
able to segment similar images with correct classes.


\paragraph{Space Carving.} We used space-carving to reconstructi
the palnts in 3D from the 2D images. The space carving
\cite{kutulakos_theory_1999} is a photogrammetric approach which uses
pictures of an object from different point of views to reconstruct
an object in 3D. To simplify the explanation we will take the example
of the original space carving paper \cite{kutulakos_theory_1999}. Let
the objective be to reconstruct a sculpture in 3D. The cameras circle
a volume of interest that contains the cube and take $N$ pictures
at regular intervals. Each picture will be pre-processed, with a
masking step: on each image the mask will give the probability that
the pixel belongs or not to the sculpture. Then, the surrounded volume
is computed virtually, and each voxel of the volume is projected onto
each masked view. If a voxel has projected onto the sculpture for each
2D-view, it is part of the 3D-sculpture. Otherwise it is part of the
background: voxel after voxel, the volume is carved into the shape of
the object. The main limitation of this technique is that the problem
is ill-posed and the reconstructed object will be a \emph{photo
hull} \cite{kutulakos_theory_1999} of the desired object, that is an
object which is the union of all objects derivable from the multi-view
observation. It can cause problems for highly curved or hollow objects,
but in the case of plants it is not a major limitation.

